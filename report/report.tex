\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}

\title{Market-Basket Analysis on the IMDB Top 1000 Movies Dataset}
\author{Ali Huseynli}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}

This project implements a frequent itemset mining system (market-basket analysis) using the IMDB Top 1000 Movies dataset published on Kaggle under the CC0-1.0 Public Domain license.

Each movie is considered as a transaction (basket), and actors listed under the columns \texttt{Star1}, \texttt{Star2}, \texttt{Star3}, and \texttt{Star4} are treated as items.

The goal is to identify frequent itemsets of actors and evaluate how the solution scales as the dataset size increases.

\section{Dataset}

The dataset is downloaded directly within the Jupyter notebook using the official Kaggle API, ensuring full reproducibility of the experiments.

The file used is \texttt{imdb\_top\_1000.csv}, which contains 1000 rows and 16 columns. Only the columns \texttt{Star1}, \texttt{Star2}, \texttt{Star3}, and \texttt{Star4} are considered for the market-basket analysis.

\section{Data Organization}

After loading the dataset with \texttt{pandas}, transactions are constructed as a list of lists:

\begin{itemize}
    \item Each movie corresponds to one transaction.
    \item Each transaction contains up to four actors.
\end{itemize}

An optional global switch allows subsampling the dataset in order to perform scalability experiments while preserving compatibility with the full dataset.

\section{Pre-processing}

Minimal preprocessing steps are applied:

\begin{itemize}
    \item Whitespace stripping from actor names.
    \item Removal of missing values (NaN).
    \item Removal of duplicate actor names within the same basket.
\end{itemize}

No additional feature engineering is performed, since the task focuses on frequent itemset mining over categorical items.

\section{Algorithm and Implementation}

The adopted solution follows the ECLAT frequent itemset mining paradigm studied during the course, relying on a vertical data representation and transaction-ID set intersections for support computation.

\subsection{Vertical Representation}

A vertical index is constructed mapping each actor to the set of transaction IDs in which the actor appears.

If $T(a)$ denotes the set of transactions containing actor $a$, the support of an itemset $\{a_1, a_2, \dots, a_k\}$ is computed as:

\[
\text{support}(\{a_1,\dots,a_k\}) = |T(a_1) \cap T(a_2) \cap \dots \cap T(a_k)|.
\]

This avoids repeated full scans of the dataset and makes support computation efficient.

\subsection{Frequent Itemset Mining}

We compute:

\begin{itemize}
    \item Frequent 1-itemsets (single actors),
    \item Frequent 2-itemsets (actor pairs).
\end{itemize}

Frequent triples were also tested but rarely appear in this dataset due to the limited basket size (maximum four actors per movie).

\section{Scalability with Data Size}

The vertical representation avoids repeated dataset scans and allows support computation through efficient set intersections. This design choice improves scalability compared to naive repeated basket scanning approaches, especially as the number of transactions increases.

To evaluate scalability, we run the same pipeline on different dataset sizes:

\[
N \in \{200, 400, 800, 1000\}.
\]

For each $N$, we measure:

\begin{itemize}
    \item Number of unique actors,
    \item Number of frequent 1-itemsets,
    \item Number of frequent 2-itemsets,
    \item Runtime.
\end{itemize}

\section{Experimental Results}

\begin{table}[h]
\centering
\begin{tabular}{rrrrr}
\toprule
$N$ & Unique Actors & Frequent Singles & Frequent Pairs & Time (s) \\
\midrule
200  & 703  & 16  & 2  & 0.0001 \\
400  & 1292 & 72  & 12 & 0.0008 \\
800  & 2268 & 204 & 42 & 0.0059 \\
1000 & 2709 & 271 & 63 & 0.0103 \\
\bottomrule
\end{tabular}
\caption{Scaling experiment results.}
\label{tab:scaling}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.75\linewidth]{runtime_plot.png}
\caption{Runtime vs dataset size.}
\label{fig:runtime}
\end{figure}

\section{Discussion}

As the dataset size increases, the number of unique actors grows, expanding the candidate search space. Consequently, the number of frequent itemsets increases and runtime grows accordingly.

Although the dataset is relatively small (1000 transactions), the observed trend is consistent with theoretical expectations: larger datasets produce larger vertical indexes and more candidate intersections.

Frequent triples are rare because each basket contains at most four actors, and the probability that the same triple appears multiple times is low under realistic support thresholds.

\section{Reproducibility}

The entire experimental pipeline is implemented in a single Jupyter notebook executable on Google Colab.

The dataset is automatically downloaded using the Kaggle API. In the public repository, placeholders are used for Kaggle credentials to avoid exposing sensitive information.

Running the notebook from start to finish reproduces all preprocessing steps, mining procedures, tables, and figures.

\section*{Declaration}

I declare that this material, which I now submit for assessment, is entirely my own work and has not been taken from the work of others, save and to the extent that such work has been cited and acknowledged within the text of my work, and including any code produced using generative AI systems. I understand that plagiarism, collusion, and copying are grave and serious offences in the university and accept the penalties that would be imposed should I engage in plagiarism, collusion or copying. This assignment, or any part of it, has not been previously submitted by me or any other person for assessment on this or any other course of study.

\end{document}
