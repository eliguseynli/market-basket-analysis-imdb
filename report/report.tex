\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}

\title{Market-Basket Analysis on the IMDB Top 1000 Movies Dataset}
\author{Ali Huseynli (65399A)}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}
This project implements a frequent itemset mining system (market-basket analysis) using the IMDB Top 1000 Movies dataset published on Kaggle (CC0-1.0 license).

Each movie is treated as a transaction (basket), and the actors listed under \texttt{Star1--Star4} are treated as items. The objective is to find frequent items and frequent pairs of items, and to discuss scalability aspects.

\section{Dataset}
The dataset is downloaded directly in the notebook using the official Kaggle API and then loaded from \texttt{imdb\_top\_1000.csv}. We use the following fields:
\begin{itemize}
    \item \texttt{Star1}, \texttt{Star2}, \texttt{Star3}, \texttt{Star4} (items),
    \item \texttt{Series\_Title} (only for readability in intermediate outputs).
\end{itemize}

\section{Data Organization}
The pipeline is implemented in a Google Colab notebook using Python 3 and Apache Spark (PySpark). After loading the CSV into a Spark DataFrame, we build an array column containing the actors for each movie. This corresponds to the transaction representation used in market-basket analysis.

\section{Pre-processing}
We apply minimal preprocessing:
\begin{itemize}
    \item Remove missing values and empty strings from actor fields.
    \item Strip whitespace from actor names.
    \item Remove duplicates inside a basket (defensive step) and sort the actors in each basket for consistency.
\end{itemize}

\section{Algorithm and Implementation (Course Technique: Spark)}
To align with the requirement of applying techniques studied in the course, the mining procedure is implemented using Spark DataFrame transformations and aggregations.

\subsection{Frequent singles}
Frequent single actors are computed by exploding the actor array and using \texttt{groupBy(actor).count()} to obtain support counts.

\subsection{Frequent pairs}
For each transaction, we generate all actor pairs (2-combinations) and count them across the dataset using Spark \texttt{groupBy(pair).count()}. This corresponds to a MapReduce-style counting pattern implemented via Spark transformations and aggregations.

\subsection{Support}
For an itemset $X$, support is defined as the number of transactions containing $X$. We report supports as absolute counts and use a minimum support threshold for filtering frequent pairs.

\section{Scalability with Data Size}
We evaluate how the proposed solution behaves as the dataset size increases by running the same Spark-based pipeline on different numbers of transactions:
\[
N \in \{200, 400, 800, 1000\}.
\]
For each $N$ we measure:
\begin{itemize}
    \item number of unique actors (distinct items),
    \item number of frequent singles (support $\geq 2$),
    \item number of frequent pairs (support $\geq 2$),
    \item runtime of the mining pipeline.
\end{itemize}

\section{Experimental Results}

\begin{table}[h]
\centering
\begin{tabular}{rrrrr}
\toprule
$N$ & Unique Actors & Frequent Singles ($\geq 2$) & Frequent Pairs ($\geq 2$) & Time (s) \\
\midrule
200  & 711  & 73  & 4   & 1.4036 \\
400  & 1297 & 189 & 30  & 0.9419 \\
800  & 2267 & 492 & 83  & 0.8583 \\
1000 & 2712 & 641 & 121 & 0.8799 \\
\bottomrule
\end{tabular}
\caption{Scaling experiment results using the Spark-based implementation.}
\label{tab:scaling}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.75\linewidth]{runtime_plot.png}
\caption{Spark runtime vs dataset size.}
\label{fig:runtime}
\end{figure}

\section{Discussion}
As expected, increasing $N$ increases the number of unique actors and the number of frequent itemsets found, since more transactions provide more opportunities for repeated co-occurrences.

Regarding runtime, results are influenced by Spark execution overhead (session warm-up, job scheduling, and JVM-related costs). In practice, the first run may be slower, and subsequent runs can benefit from already-initialized execution components. Despite this overhead, the experiment demonstrates that the mining pipeline can be expressed as scalable distributed operations (Spark transformations and aggregations), which is consistent with the course focus.

\section{Reproducibility}
The full pipeline is implemented in a single Jupyter notebook executable on Google Colab. The dataset is downloaded via the Kaggle API. In the public version, Kaggle credentials are not exposed and placeholders are used.

\section*{Declaration}
I declare that this material, which I now submit for assessment, is entirely my own work and has not been taken from the work of others, save and to the extent that such work has been cited and acknowledged within the text of my work, and including any code produced using generative AI systems. I understand that plagiarism, collusion, and copying are grave and serious offences in the university and accept the penalties that would be imposed should I engage in plagiarism, collusion or copying. This assignment, or any part of it, has not been previously submitted by me or any other person for assessment on this or any other course of study.

\end{document}
